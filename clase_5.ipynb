{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Ejemplo sencillo de PCA en un conjunto de datos bivariado\n",
    "\n",
    "## Importar las bibliotecas necesarias\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Generar un conjunto de datos bivariado\n",
    "np.random.seed(42)\n",
    "x = np.random.normal(0, 1, 100)\n",
    "y = 2 * x + np.random.normal(0, 1, 100)\n",
    "data = np.column_stack((x, y))\n",
    "\n",
    "# Visualización inicial de los datos\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(data[:, 0], data[:, 1], alpha=0.7, label='Datos originales')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "plt.title('Conjunto de datos original')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# Escalar los datos\n",
    "scaler = StandardScaler()\n",
    "data_scaled = scaler.fit_transform(data)\n",
    "\n",
    "# Aplicar PCA\n",
    "pca = PCA(n_components=2)\n",
    "data_pca = pca.fit_transform(data_scaled)\n",
    "\n",
    "# Visualizar los datos transformados (componentes principales)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(data_pca[:, 0], data_pca[:, 1], alpha=0.7, label='Datos transformados (PCA)')\n",
    "plt.axhline(0, color='red', linestyle='--', label='Ejes principales')\n",
    "plt.axvline(0, color='blue', linestyle='--')\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.title('Datos reducidos con PCA')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# Interpretación de los resultados\n",
    "print(\"Varianza explicada por cada componente principal:\", pca.explained_variance_ratio_)\n",
    "print(\"Varianza total explicada:\", np.sum(pca.explained_variance_ratio_))\n",
    "\n",
    "# Conclusión\n",
    "print(\"La reducción de dimensionalidad con PCA ha proyectado los datos en un nuevo espacio donde la mayor varianza está en la primera componente principal.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook: PCA aplicado a Segmentación de Clientes\n",
    "\n",
    "## Introducción\n",
    "En este notebook, exploraremos cómo aplicar PCA (Análisis de Componentes Principales) para reducir la dimensionalidad del \"Mall Customer Segmentation Data\" y visualizar el impacto de esta técnica en el análisis de datos.\n",
    "\n",
    "### Objetivos\n",
    "1. Comprender cómo PCA transforma los datos para reducir dimensiones mientras conserva la varianza principal.\n",
    "2. Aplicar PCA al conjunto de datos de segmentación de clientes.\n",
    "3. Visualizar los datos reducidos y analizar los resultados.\n",
    "\n",
    "---\n",
    "\n",
    "## Estructura de la Clase\n",
    "\n",
    "### 1. Introducción al PCA (30 minutos)\n",
    "\n",
    "#### ¿Qué es PCA?\n",
    "- Técnica de reducción de dimensionalidad.\n",
    "- Proyección de datos en un espacio de menor dimensión preservando la máxima varianza.\n",
    "\n",
    "#### Motivación\n",
    "- Manejo de conjuntos de datos con muchas variables.\n",
    "- Reducción de ruido y mejora de la interpretabilidad.\n",
    "\n",
    "#### Conceptos clave\n",
    "- **Varianza**: Representa la información de los datos.\n",
    "- **Autovectores y autovalores**: Determinan las direcciones principales y su importancia.\n",
    "- **Componentes principales**: Nuevas variables no correlacionadas.\n",
    "\n",
    "---\n",
    "\n",
    "## Implementación Práctica\n",
    "\n",
    "### Paso 1: Importar bibliotecas necesarias\n",
    "```python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "```\n",
    "\n",
    "### Paso 2: Cargar los datos\n",
    "```python\n",
    "url = \"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/mall_customers.csv\"\n",
    "data = pd.read_csv(url)\n",
    "data.head()\n",
    "```\n",
    "\n",
    "### Paso 3: Explorar los datos\n",
    "```python\n",
    "print(data.info())\n",
    "print(data.describe())\n",
    "\n",
    "# Convertir género en valores numéricos para simplificar\n",
    "data['Gender'] = data['Gender'].map({'Male': 0, 'Female': 1})\n",
    "\n",
    "# Seleccionar características relevantes\n",
    "df_features = data[['Gender', 'Age', 'Annual Income (k$)', 'Spending Score (1-100)']]\n",
    "```\n",
    "\n",
    "### Paso 4: Escalar los datos\n",
    "```python\n",
    "scaler = StandardScaler()\n",
    "data_scaled = scaler.fit_transform(df_features)\n",
    "```\n",
    "\n",
    "### Paso 5: Aplicar PCA\n",
    "```python\n",
    "pca = PCA()\n",
    "principal_components = pca.fit_transform(data_scaled)\n",
    "\n",
    "# Variancia explicada acumulativa\n",
    "explained_variance = np.cumsum(pca.explained_variance_ratio_)\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(range(1, len(explained_variance) + 1), explained_variance, marker='o', linestyle='--')\n",
    "plt.xlabel('Número de Componentes Principales')\n",
    "plt.ylabel('Varianza Explicada Acumulativa')\n",
    "plt.title('Método del Codo para PCA')\n",
    "plt.show()\n",
    "\n",
    "# Seleccionar las dos primeras componentes principales\n",
    "pca = PCA(n_components=2)\n",
    "principal_components_2d = pca.fit_transform(data_scaled)\n",
    "```\n",
    "\n",
    "### Paso 6: Visualizar los datos reducidos\n",
    "```python\n",
    "pca_df = pd.DataFrame(data=principal_components_2d, columns=['PC1', 'PC2'])\n",
    "pca_df['Spending Score'] = data['Spending Score (1-100)']\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(x='PC1', y='PC2', hue='Spending Score', data=pca_df, palette='viridis')\n",
    "plt.title('Visualización de Datos Reducidos con PCA')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "### Paso 7: Conclusión\n",
    "```python\n",
    "print(\"Reducción de dimensionalidad completada. Visualización lista para análisis.\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Ejercicio\n",
    "- Cambiar el número de componentes principales y observar el impacto en la varianza explicada.\n",
    "- Intentar aplicar clustering (como KMeans) sobre los datos reducidos para segmentar los clientes.\n",
    "\n",
    "---\n",
    "\n",
    "## Preguntas del Quiz\n",
    "1. ¿Qué significa PCA y cuál es su propósito principal?\n",
    "2. ¿Por qué es útil reducir la dimensionalidad en un conjunto de datos?\n",
    "3. ¿Qué representan los autovectores en PCA?\n",
    "4. ¿Qué se entiende por varianza explicada acumulativa?\n",
    "5. ¿Cuál es el criterio para elegir el número óptimo de componentes principales?\n",
    "6. ¿Cómo se preprocesan los datos antes de aplicar PCA?\n",
    "7. ¿Qué ventaja tiene la reducción de ruido al usar PCA?\n",
    "8. ¿Qué relación tiene PCA con los datos originales después de la transformación?\n",
    "9. ¿Cómo se visualizan típicamente los datos después de aplicar PCA?\n",
    "10. ¿Cuál es el impacto de no escalar los datos antes de aplicar PCA?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook: PCA aplicado a Segmentación de Clientes\n",
    "\n",
    "## Introducción\n",
    "En este notebook, exploraremos cómo aplicar PCA (Análisis de Componentes Principales) para reducir la dimensionalidad del \"Mall Customer Segmentation Data\" y visualizar el impacto de esta técnica en el análisis de datos.\n",
    "\n",
    "### Objetivos\n",
    "1. Comprender cómo PCA transforma los datos para reducir dimensiones mientras conserva la varianza principal.\n",
    "2. Aplicar PCA al conjunto de datos de segmentación de clientes.\n",
    "3. Visualizar los datos reducidos y analizar los resultados.\n",
    "\n",
    "---\n",
    "\n",
    "# Paso 1: Importar bibliotecas necesarias\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Paso 2: Cargar los datos\n",
    "url = \"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/mall_customers.csv\"\n",
    "data = pd.read_csv(url)\n",
    "data.head()\n",
    "\n",
    "# Paso 3: Explorar los datos\n",
    "print(data.info())\n",
    "print(data.describe())\n",
    "\n",
    "# Convertir género en valores numéricos para simplificar\n",
    "data['Gender'] = data['Gender'].map({'Male': 0, 'Female': 1})\n",
    "\n",
    "# Seleccionar características relevantes\n",
    "df_features = data[['Gender', 'Age', 'Annual Income (k$)', 'Spending Score (1-100)']]\n",
    "\n",
    "# Paso 4: Escalar los datos\n",
    "scaler = StandardScaler()\n",
    "data_scaled = scaler.fit_transform(df_features)\n",
    "\n",
    "# Paso 5: Aplicar PCA\n",
    "pca = PCA()\n",
    "principal_components = pca.fit_transform(data_scaled)\n",
    "\n",
    "# Variancia explicada acumulativa\n",
    "explained_variance = np.cumsum(pca.explained_variance_ratio_)\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(range(1, len(explained_variance) + 1), explained_variance, marker='o', linestyle='--')\n",
    "plt.xlabel('Número de Componentes Principales')\n",
    "plt.ylabel('Varianza Explicada Acumulativa')\n",
    "plt.title('Método del Codo para PCA')\n",
    "plt.show()\n",
    "\n",
    "# Seleccionar las dos primeras componentes principales\n",
    "pca = PCA(n_components=2)\n",
    "principal_components_2d = pca.fit_transform(data_scaled)\n",
    "\n",
    "# Paso 6: Visualizar los datos reducidos\n",
    "pca_df = pd.DataFrame(data=principal_components_2d, columns=['PC1', 'PC2'])\n",
    "pca_df['Spending Score'] = data['Spending Score (1-100)']\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(x='PC1', y='PC2', hue='Spending Score', data=pca_df, palette='viridis')\n",
    "plt.title('Visualización de Datos Reducidos con PCA')\n",
    "plt.show()\n",
    "\n",
    "# Paso 7: Conclusión\n",
    "print(\"Reducción de dimensionalidad completada. Visualización lista para análisis.\")\n",
    "\n",
    "---\n",
    "\n",
    "# Ejercicio\n",
    "- Cambiar el número de componentes principales y observar el impacto en la varianza explicada.\n",
    "- Intentar aplicar clustering (como KMeans) sobre los datos reducidos para segmentar los clientes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
